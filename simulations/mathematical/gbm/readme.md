## GBM

Our data science team makes ample use of "Gradient Boosting Machines" (GBMs).

T. Q. Chen's paper ["Boosted Trees"](http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf) explains the GBM pretty well with an in-depth mathematical grounding. (You may also like to watch [Fei's Talk on the paper](https://www.youtube.com/watch?v=031j956LzII).)

Also watch this video on the [xgboost algorithm](https://www.youtube.com/watch?v=X47SGnTMZIU) by Tong He.


## Exercise Questions

These questions are pretty hard, but make a decent effort on your own to try and take a stab into the more technical parts of machine learning:

1. What are the hyper parameters to tune for tree-based method?   
2. Are trees sensitive to scaling or monotonic transformations? Why or why not?
3. Explain how trees are grown in the context of *information gain*.
4. What is L1/L2 Regularization? Where within the GBM algorithim does one impose it?
5. When do you stop training each tree and add more trees to the forest?
6. Explain how the gradient and hassien is used within the algorithim.    
7. Derive the gradient and hessian of the binary classification entropy loss function.
8. Any useful notes to add?
