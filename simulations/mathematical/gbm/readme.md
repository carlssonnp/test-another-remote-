## GBM

Our data science team makes ample use of "Gradient Boosting Machines" (GBMs).

T. Q. Chen's paper ["Boosted Trees"](http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf) explains the GBM pretty well with an in-depth mathematical grounding. (You may also like to watch [Fei's Talk on the paper](https://www.youtube.com/watch?v=031j956LzII).)

Also watch this video on the [xgboost algorithm](https://www.youtube.com/watch?v=X47SGnTMZIU) by Tong He.


## Exercise Questions

Completeing all these questions is not required but it is a good idea to take a stab at it if you can. Feel free to ask your gaurdian or Fei for help.

1) What are the hyper parameters to tune for tree-based method?    
2) Are trees sensitive to scaling or monotonic transformations? Why or why not?  
3) Explain how trees are grown in the context of *information gain*.
4) What is L1/L2 Regularization? Where within the GBM algorithim does one impose it?
6) When do you stop training each tree and add more trees to the forest?
5) Explain how the gradient and hassien is used within the algorithim.    
7) Derive the gradient and hessian of the binary classification entropy loss function.
8) Any useful notes to add?
